{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Anukool Purohit\n",
    "### Nueral Network Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relu Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = (Z>0)*Z\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:  \n",
      "[[ 1.43479746]\n",
      " [ 0.24563544]\n",
      " [ 1.25322491]\n",
      " [-0.69529393]\n",
      " [ 1.91073356]]\n",
      "Relu: \n",
      "[[ 1.43479746]\n",
      " [ 0.24563544]\n",
      " [ 1.25322491]\n",
      " [-0.        ]\n",
      " [ 1.91073356]]\n",
      "True\n",
      "[[ 1.43479746]\n",
      " [ 0.24563544]\n",
      " [ 1.25322491]\n",
      " [-0.69529393]\n",
      " [ 1.91073356]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.random.randn(5,1)\n",
    "print(\"Z:  \")\n",
    "print(Z)\n",
    "print(\"Relu: \")\n",
    "A, cache = relu(Z)\n",
    "print(A)\n",
    "print(Z.shape == A.shape)\n",
    "print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-1 * Z))\n",
    "    assert Z.shape == A.shape\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.g:  \n",
      "[[-1.10818989]\n",
      " [-0.35323388]\n",
      " [ 0.72687068]\n",
      " [ 0.71237052]\n",
      " [ 0.644083  ]]\n",
      "Sig: \n",
      "[[0.2482085 ]\n",
      " [0.41259843]\n",
      " [0.67411819]\n",
      " [0.67092475]\n",
      " [0.65567584]]\n",
      "True\n",
      "[[-1.10818989]\n",
      " [-0.35323388]\n",
      " [ 0.72687068]\n",
      " [ 0.71237052]\n",
      " [ 0.644083  ]]\n"
     ]
    }
   ],
   "source": [
    "eg = np.random.randn(5,1)\n",
    "print(\"E.g:  \")\n",
    "print(eg)\n",
    "print(\"Sig: \")\n",
    "sig, cache = sigmoid(eg)\n",
    "print(sig)\n",
    "print(eg.shape == sig.shape)\n",
    "print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relu Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_gradient(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy= True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert dZ.shape == Z.shape\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.01486686]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.77105174]]\n"
     ]
    }
   ],
   "source": [
    "dA = np.random.randn(4,1)\n",
    "cache = np.random.randn(4,1)\n",
    "dZ = relu_gradient(dA, cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-1 * Z))\n",
    "    \n",
    "    dZ = dA * s * (1-s)\n",
    "    assert dZ.shape == Z.shape\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25744408]\n",
      " [ 0.46807984]\n",
      " [-0.38191486]\n",
      " [-0.06783684]]\n"
     ]
    }
   ],
   "source": [
    "dA = np.random.randn(4,1)\n",
    "cache = np.random.randn(4,1)\n",
    "dZ = sigmoid_gradient(dA, cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for NN arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        assert parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1])\n",
    "        assert parameters['b' + str(l)].shape == (layer_dims[l], 1)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00576118 -0.00745127  0.01388894  0.00514156 -0.00992654]\n",
      " [ 0.00343834  0.00729609 -0.00677551  0.00559101 -0.01244712]\n",
      " [-0.00908862  0.01085418 -0.00407766  0.00475849 -0.00043449]\n",
      " [-0.01178733 -0.0084548  -0.00187678  0.00924892  0.0052422 ]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters([5,4,2])\n",
    "print(parameters['W1'])\n",
    "print(parameters['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert Z.shape == (W.shape[0], A.shape[1])\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "[[ 0.76345615]\n",
      " [-2.20648138]\n",
      " [ 1.22391736]]\n",
      "Z: \n",
      "[[-0.00619592]\n",
      " [ 0.01568658]\n",
      " [ 0.01808807]\n",
      " [-0.00821288]]\n",
      "Cache: \n",
      "    A: \n",
      "[[ 0.76345615]\n",
      " [-2.20648138]\n",
      " [ 1.22391736]]\n",
      "    W: \n",
      "[[ 0.00480866 -0.0020054  -0.01167727]\n",
      " [-0.00210598 -0.00211688  0.01031406]\n",
      " [ 0.00082289 -0.01540136 -0.01350008]\n",
      " [-0.00314927  0.01165996  0.01627473]]\n",
      "    b: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters([3,4,5])\n",
    "A = np.random.randn(3,1)\n",
    "print(\"A: \")\n",
    "print(A)\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "Z, cache = linear_forward(A, W, b)\n",
    "print(\"Z: \")\n",
    "print(Z)\n",
    "print(\"Cache: \")\n",
    "print(\"    A: \")\n",
    "print((cache[0]))\n",
    "print(\"    W: \")\n",
    "print((cache[1]))\n",
    "print(\"    b: \")\n",
    "print((cache[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1])\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prev: \n",
      "[[-1.01538002]\n",
      " [-1.36762589]\n",
      " [ 0.72259254]]\n",
      "Z: \n",
      "[[ 0.00235079]\n",
      " [-0.01272416]\n",
      " [-0.00199401]\n",
      " [ 0.00022851]]\n",
      "A: \n",
      "[[ 0.00235079]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [ 0.00022851]]\n",
      "Cache: \n",
      "((array([[-1.01538002],\n",
      "       [-1.36762589],\n",
      "       [ 0.72259254]]), array([[-3.03515004e-02,  1.40313670e-02, -1.28396723e-02],\n",
      "       [ 5.52227312e-03,  1.09084145e-03, -7.78460027e-03],\n",
      "       [ 1.80848074e-03,  9.14709493e-05, -4.51389186e-05],\n",
      "       [-5.25879580e-03,  1.00833974e-02,  1.20111295e-02]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 0.00235079],\n",
      "       [-0.01272416],\n",
      "       [-0.00199401],\n",
      "       [ 0.00022851]]))\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters([3,4,5])\n",
    "A_prev = np.random.randn(3,1)\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "A, cache = linear_activation_forward(A_prev, W, b, activation='relu')\n",
    "print(\"A_prev: \")\n",
    "print(A_prev)\n",
    "print(\"Z: \")\n",
    "print(cache[1])\n",
    "print(\"A: \")\n",
    "print(A)\n",
    "print(\"Cache: \")\n",
    "print(cache)\n",
    "# print(\"    A: \")\n",
    "# print((cache[0][0]))\n",
    "# print(\"    W: \")\n",
    "# print((cache[0][1]))\n",
    "# print(\"    b: \")\n",
    "# print((cache[0][2]))\n",
    "# print(\"    Z: \")\n",
    "# print((cache[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_pass(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A, cache = linear_activation_forward(A, parameters['W' + str(l)], parameters['b'+ str(l)], activation = 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL:  \n",
      "[[0.5]\n",
      " [0.5]]\n",
      "caches:\n",
      "[((array([[ 0.22128863],\n",
      "       [ 0.01460948],\n",
      "       [-1.73562033]]), array([[-0.01079376,  0.00559298, -0.00841057],\n",
      "       [-0.01379845, -0.00410019, -0.02194486],\n",
      "       [-0.01298333, -0.01743026, -0.01620041],\n",
      "       [-0.00287286, -0.00293349,  0.01160664],\n",
      "       [-0.0025644 , -0.00965737, -0.00845371]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 0.01229074],\n",
      "       [ 0.0349746 ],\n",
      "       [ 0.02499005],\n",
      "       [-0.02082331],\n",
      "       [ 0.01396387]])), ((array([[ 0.01229074],\n",
      "       [ 0.0349746 ],\n",
      "       [ 0.02499005],\n",
      "       [-0.        ],\n",
      "       [ 0.01396387]]), array([[-0.00191052,  0.02363347,  0.00208426, -0.00907032,  0.00159932],\n",
      "       [ 0.00208367, -0.00015643, -0.00479322, -0.00405261,  0.0159644 ],\n",
      "       [ 0.00378811,  0.0084982 ,  0.00020738,  0.01366372, -0.00703907],\n",
      "       [-0.00570633, -0.00240464, -0.00407209,  0.00059905,  0.00034705],\n",
      "       [ 0.00409462,  0.02296791,  0.00872095,  0.00371393, -0.01014644],\n",
      "       [ 0.00775018,  0.01173762, -0.0095653 ,  0.00225943, -0.01638758],\n",
      "       [-0.01346098,  0.00033355, -0.00269628, -0.01643624,  0.00204984],\n",
      "       [-0.0019144 , -0.00447039,  0.00083354,  0.01029264,  0.01907477]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 8.77507899e-04],\n",
      "       [ 1.23280860e-04],\n",
      "       [ 2.50669393e-04],\n",
      "       [-2.51152057e-04],\n",
      "       [ 9.29872937e-04],\n",
      "       [ 3.79024540e-05],\n",
      "       [-1.92536127e-04],\n",
      "       [ 1.07308310e-04]])), ((array([[ 8.77507899e-04],\n",
      "       [ 1.23280860e-04],\n",
      "       [ 2.50669393e-04],\n",
      "       [-0.00000000e+00],\n",
      "       [ 9.29872937e-04],\n",
      "       [ 3.79024540e-05],\n",
      "       [-0.00000000e+00],\n",
      "       [ 1.07308310e-04]]), array([[ 1.75125015e-02, -1.06603195e-02, -2.04346390e-05,\n",
      "        -1.41529747e-02,  2.03463989e-02,  4.64918092e-03,\n",
      "        -1.35094871e-03, -1.20850401e-02],\n",
      "       [ 7.18651036e-03, -1.49885081e-03, -1.34823878e-02,\n",
      "         1.69975211e-02,  1.70574988e-03,  3.88832180e-03,\n",
      "        -4.72947924e-03,  1.57210120e-03],\n",
      "       [ 6.01609536e-04,  2.35743636e-02, -2.20469162e-03,\n",
      "        -2.36642179e-03, -9.41232678e-03, -2.71508624e-03,\n",
      "        -3.58611833e-03, -2.42246291e-02],\n",
      "       [-1.19059978e-02,  2.88115794e-03, -1.73723195e-02,\n",
      "        -1.12994479e-02,  4.10312725e-03, -6.16912135e-03,\n",
      "         2.15586389e-03, -1.10045883e-03],\n",
      "       [ 5.57382379e-03,  1.17835412e-02,  1.87665470e-02,\n",
      "         3.34123879e-03, -1.28707260e-02,  7.09092621e-03,\n",
      "        -8.29999504e-03, -1.76904849e-02],\n",
      "       [-1.33227700e-02,  5.15160741e-03, -2.05682233e-02,\n",
      "        -1.44525567e-03, -1.04100480e-02, -1.17310074e-02,\n",
      "        -1.34095089e-02,  4.26456527e-05]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 3.18469785e-05],\n",
      "       [ 4.64402512e-06],\n",
      "       [-8.57314417e-06],\n",
      "       [-1.09836505e-05],\n",
      "       [-2.54975384e-06],\n",
      "       [-2.63316450e-05]])), ((array([[ 3.18469785e-05],\n",
      "       [ 4.64402512e-06],\n",
      "       [-0.00000000e+00],\n",
      "       [-0.00000000e+00],\n",
      "       [-0.00000000e+00],\n",
      "       [-0.00000000e+00]]), array([[ 0.01835508, -0.01368587,  0.00992229,  0.00092843, -0.01709195,\n",
      "         0.02177552],\n",
      "       [ 0.00561055, -0.01462683, -0.015008  ,  0.0039215 ,  0.00142494,\n",
      "         0.01814366],\n",
      "       [ 0.00698096, -0.00770727,  0.00845608, -0.00602826,  0.0114732 ,\n",
      "         0.00749999]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[5.20996317e-07],\n",
      "       [1.10751759e-07],\n",
      "       [1.86529725e-07]])), ((array([[5.20996317e-07],\n",
      "       [1.10751759e-07],\n",
      "       [1.86529725e-07]]), array([[ 0.02230252, -0.00297721,  0.00748076],\n",
      "       [-0.00367272,  0.0074341 , -0.00849158]]), array([[0.],\n",
      "       [0.]])), array([[ 1.26851808e-08],\n",
      "       [-2.67406725e-09]]))]\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [3,5,8,6,3,2]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "X = np.random.randn(3,1)\n",
    "AL, caches = full_forward_pass(X, parameters)\n",
    "print(\"AL:  \")\n",
    "print(AL)\n",
    "print(\"caches:\")\n",
    "print(caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1- AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert cost.shape == ()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL: \n",
      "[[0.0294794 ]\n",
      " [0.04141099]\n",
      " [0.02221734]\n",
      " [0.11293798]]\n",
      "Y:  \n",
      "[[0.27483815]\n",
      " [0.80474374]\n",
      " [1.80326858]\n",
      " [0.99958699]]\n",
      "Cost:\n",
      "12.587824480229388\n"
     ]
    }
   ],
   "source": [
    "AL = np.absolute(np.random.randn(4,1)*0.1)\n",
    "Y = np.absolute(np.random.randn(4,1))\n",
    "print(\"AL: \")\n",
    "print(AL)\n",
    "print(\"Y:  \")\n",
    "print(Y)\n",
    "cost = compute_cost(AL,Y)\n",
    "print(\"Cost:\")\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev , W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m)* np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)* np.sum(dZ, axis = 1, keepdims= True)\n",
    "    \n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "    assert dA_prev  == A_prev.shape\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_gradient(dA, activation_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_gradient(dA, activation_cache)\n",
    "    \n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_prop(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(Al.shape)\n",
    "    \n",
    "    # Initialize Backprop with dAL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Final layer with sigmoid activation\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L-1)], grads[\"db\" + str(L-1)] = linear_activation_backward(dAL, current_cache, activation= 'sigmoid')\n",
    "    \n",
    "    # Loop from L-2 layer to layer 0\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+ str(l+1)], current_cache, activation= 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = parameters //2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
